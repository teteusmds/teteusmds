{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM+Bx6hokwsg/JNF3E/gEMt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teteusmds/teteusmds/blob/main/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1)"
      ],
      "metadata": {
        "id": "qpocv_bct-Wz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQlpjDeDs0r0",
        "outputId": "925115c2-26d8-4029-f19b-35d816933b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output para [1,1]: 1\n",
            "Output para [0,1]: 0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, epochs=1000, learning_rate=0.01):\n",
        "        self.weights = np.zeros(input_size)\n",
        "        self.bias = 0\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        linear_output = np.dot(inputs, self.weights) + self.bias\n",
        "        return 1 if linear_output > 0 else 0\n",
        "\n",
        "    def train(self, training_inputs, labels):\n",
        "        for _ in range(self.epochs):\n",
        "            for inputs, label in zip(training_inputs, labels):\n",
        "                prediction = self.predict(inputs)\n",
        "                update = self.learning_rate * (label - prediction)\n",
        "                self.weights += update * inputs\n",
        "                self.bias += update\n",
        "\n",
        "# Dados de treinamento para o operador XOR\n",
        "training_inputs = []\n",
        "training_inputs.append(np.array([1, 1]))\n",
        "training_inputs.append(np.array([1, 0]))\n",
        "training_inputs.append(np.array([0, 1]))\n",
        "training_inputs.append(np.array([0, 0]))\n",
        "\n",
        "labels = np.array([0, 1, 1, 0])  # Tabela verdade XOR\n",
        "\n",
        "# Inicializando e treinando o Perceptron\n",
        "perceptron = Perceptron(2)\n",
        "perceptron.train(training_inputs, labels)\n",
        "\n",
        "# Testando para [1,1] e [0,1]\n",
        "inputs1 = np.array([1, 1])\n",
        "p1 = perceptron.predict(inputs1)\n",
        "print(f\"Output para [1,1]: {p1}\")\n",
        "\n",
        "inputs2 = np.array([0, 1])\n",
        "p2 = perceptron.predict(inputs2)\n",
        "print(f\"Output para [0,1]: {p2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados:\n",
        "# Saída para [1,1]: O Perceptron simples retorna sempre 0 para este caso, porque ele não consegue capturar a relação não linear necessária.\n",
        "\n",
        "# Saída para [0,1]: O Perceptron pode retornar 1 ou outro valor inconsistente com a lógica XOR, dependendo do peso inicial e do treinamento.\n",
        "\n"
      ],
      "metadata": {
        "id": "BTrorYM_t6Sm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2)"
      ],
      "metadata": {
        "id": "Oe7RQu9xuEdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# O resultado do Perceptron deu errado na tentativa de resolver o operador XOR devido a uma limitação fundamental do modelo\n",
        "\n",
        " # O problema do XOR não é linearmente separável\n",
        " # Para resolver o XOR com um modelo linear, é preciso transformar o problema. Uma rede neural multicamada, por exemplo, cria combinações não lineares dos dados para que eles se tornem separáveis em um espaço maior."
      ],
      "metadata": {
        "id": "ZdSL6N7-uOyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3)"
      ],
      "metadata": {
        "id": "SQPz8hOqvwfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron_to_Adaline(object):\n",
        "\n",
        "    def __init__(self, no_of_inputs, threshold=0.5, nIterations=100, learning_rate=0.01):\n",
        "        self.nIterations = nIterations\n",
        "        self.threshold = threshold\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = np.zeros(no_of_inputs)\n",
        "        self.bias = 0  # Adicionando um termo de bias para o Adaline\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        summation = np.dot(inputs, self.weights) + self.bias\n",
        "        activation = self.sigmoid(summation)\n",
        "        return activation\n",
        "\n",
        "    def train(self, training_inputs, labels):\n",
        "          for _ in range(self.nIterations):\n",
        "            for inputs, label in zip(training_inputs, labels):\n",
        "\n",
        "                prediction = self.predict(inputs)\n",
        "\n",
        "                error = label - prediction\n",
        "                delta = self.learning_rate * error * inputs\n",
        "\n",
        "                self.weights += delta\n",
        "                self.bias += self.learning_rate * error\n"
      ],
      "metadata": {
        "id": "A-_XpXDUwR81"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados de treinamento para o XOR\n",
        "training_inputs = [\n",
        "    np.array([1, 1]),\n",
        "    np.array([1, 0]),\n",
        "    np.array([0, 1]),\n",
        "    np.array([0, 0])\n",
        "]\n",
        "labels = np.array([0, 1, 1, 0])\n",
        "\n",
        "\n",
        "adaline = Perceptron_to_Adaline(no_of_inputs=2, nIterations=10000, learning_rate=0.1)\n",
        "\n",
        "\n",
        "adaline.train(training_inputs, labels)\n",
        "\n",
        "test_inputs = [np.array([1, 1]), np.array([0, 1])]\n",
        "for test in test_inputs:\n",
        "    prediction = adaline.predict(test)\n",
        "    print(f\"Entrada: {test}, Previsão: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2LLZSm1wXHt",
        "outputId": "0b000c5f-cc89-43ac-a43c-72dc1275d728"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrada: [1 1], Previsão: 0.5128176319164417\n",
            "Entrada: [0 1], Previsão: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4)"
      ],
      "metadata": {
        "id": "zeG2bA5iyVYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_data():\n",
        "    url = '/content/diabetes.csv'\n",
        "    df = pd.read_csv(url)\n",
        "    data = df[df.columns[:-1]]\n",
        "    normalized_data = (data - data.min()) / (data.max() - data.min())\n",
        "\n",
        "    labels = df[df.columns[-1]]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(normalized_data, labels, test_size=0.2, random_state=0)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, n_iter=100, learning_rate=0.01, threshold=0.5):\n",
        "        self.n_iter = n_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.threshold = threshold\n",
        "        self.weights = None\n",
        "\n",
        "    def train(self, X, y):\n",
        "        self.weights = np.zeros(X.shape[1] + 1)\n",
        "        for _ in range(self.n_iter):\n",
        "            for xi, target in zip(X, y):\n",
        "                update = self.learning_rate * (target - self.predict(xi))\n",
        "\n",
        "                self.weights[1:] += update * xi\n",
        "                self.weights[0] += update\n",
        "        return self.weights\n",
        "\n",
        "    def predict(self, X):\n",
        "        activation = np.dot(X, self.weights[1:]) + self.weights[0]\n",
        "        return 1 if activation >= self.threshold else 0\n",
        "\n",
        "class Adaline:\n",
        "    def __init__(self, n_iter=100, learning_rate=0.01, threshold=0.5):\n",
        "        self.n_iter = n_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.threshold = threshold\n",
        "        self.weights = None\n",
        "\n",
        "    def train(self, X, y):\n",
        "        self.weights = np.zeros(X.shape[1] + 1)\n",
        "        for _ in range(self.n_iter):\n",
        "            for xi, target in zip(X, y):\n",
        "\n",
        "                prediction = self.predict(xi)\n",
        "                error = target - prediction\n",
        "                self.weights[1:] += self.learning_rate * error * xi\n",
        "                self.weights[0] += self.learning_rate * error\n",
        "        return self.weights\n",
        "\n",
        "    def predict(self, X):\n",
        "        activation = np.dot(X, self.weights[1:]) + self.weights[0]\n",
        "        return 1 if activation >= self.threshold else 0\n",
        "\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test, learning_rate, threshold):\n",
        "    model.learning_rate = learning_rate\n",
        "    model.threshold = threshold\n",
        "    model.train(X_train.values, y_train.values)\n",
        "\n",
        "    correct_predictions = 0\n",
        "    for xi, target in zip(X_test.values, y_test.values):\n",
        "        prediction = model.predict(xi)\n",
        "        if prediction == target:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / len(y_test) * 100\n",
        "    return accuracy\n",
        "\n",
        "training_inputs, test_inputs, training_labels, test_labels = load_data()\n",
        "\n",
        "perceptron_results = {}\n",
        "adaline_results = {}\n",
        "\n",
        "configurations = [\n",
        "    (0.01, 0.2),\n",
        "    (0.1, 0.2),\n",
        "    (0.01, 0.5),\n",
        "    (0.1, 0.5)\n",
        "]\n",
        "\n",
        "for learning_rate, threshold in configurations:\n",
        "    perceptron = Perceptron(learning_rate=learning_rate, threshold=threshold)\n",
        "    accuracy = evaluate_model(perceptron, training_inputs, test_inputs, training_labels, test_labels, learning_rate, threshold)\n",
        "    perceptron_results[(learning_rate, threshold)] = accuracy\n",
        "\n",
        "for learning_rate, threshold in configurations:\n",
        "    adaline = Adaline(learning_rate=learning_rate, threshold=threshold)\n",
        "    accuracy = evaluate_model(adaline, training_inputs, test_inputs, training_labels, test_labels, learning_rate, threshold)\n",
        "    adaline_results[(learning_rate, threshold)] = accuracy\n",
        "\n",
        "# Exibe os resultados\n",
        "print(\"Taxa de acerto para o Perceptron:\")\n",
        "for config, accuracy in perceptron_results.items():\n",
        "    print(f\"Taxa de Aprendizado = {config[0]}, Limiar = {config[1]}: {accuracy:.2f}%\")\n",
        "\n",
        "print(\"\\nTaxa de acerto para o Adaline:\")\n",
        "for config, accuracy in adaline_results.items():\n",
        "    print(f\"Taxa de Aprendizado = {config[0]}, Limiar = {config[1]}: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inKuzF3yyXNj",
        "outputId": "06d41a99-b3c5-47e4-a460-64a4d7babf4c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taxa de acerto para o Perceptron:\n",
            "Taxa de Aprendizado = 0.01, Limiar = 0.2: 70.78%\n",
            "Taxa de Aprendizado = 0.1, Limiar = 0.2: 70.78%\n",
            "Taxa de Aprendizado = 0.01, Limiar = 0.5: 71.43%\n",
            "Taxa de Aprendizado = 0.1, Limiar = 0.5: 70.78%\n",
            "\n",
            "Taxa de acerto para o Adaline:\n",
            "Taxa de Aprendizado = 0.01, Limiar = 0.2: 70.78%\n",
            "Taxa de Aprendizado = 0.1, Limiar = 0.2: 70.78%\n",
            "Taxa de Aprendizado = 0.01, Limiar = 0.5: 71.43%\n",
            "Taxa de Aprendizado = 0.1, Limiar = 0.5: 70.78%\n"
          ]
        }
      ]
    }
  ]
}